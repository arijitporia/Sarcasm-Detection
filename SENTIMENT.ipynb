{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PU4GDhQ39UJZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from  nltk.stem import SnowballStemmer\n",
        "import numpy as np\n",
        "import re\n",
        "import gensim\n",
        "from collections import defaultdict\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjs4upxNAHaj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVCbkzCk9lsG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z]+\"\n",
        "DATASET_ENCODING = \"ISO-8859-1\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PLnjFX_ALjD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop_words = stopwords.words(\"english\")\n",
        "stemmer = SnowballStemmer(\"english\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1roqfoDz9wWC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('/content/drive/My Drive/tweets_sentiment.csv',encoding=DATASET_ENCODING, header=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAO1ltpvVtE7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "abbr={\"ain't\": \"is not\", \"isn't\": 'is not',\"cannot\": \"can not\", \"aren't\": \"are not\", \"can't\": \"can not\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",\n",
        "                \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\", \"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\",\n",
        "                \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\", \"I'm\": \"I am\",\n",
        "                \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\", \"i'll've\": \"i will have\", \"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
        "                \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\", \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\n",
        "                \"mightn't\": \"might not\", \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n",
        "                \"needn't've\": \"need not have\", \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\",\n",
        "                \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
        "                \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\", \"so's\": \"so as\", \"this's\": \"this is\", \"that'd\": \"that would\",\n",
        "                \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\", \"they'd\": \"they would\",\n",
        "                \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\",\n",
        "                \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\",\n",
        "                \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\",\n",
        "                \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
        "                \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"wont\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\",\n",
        "                \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\", \"y'all'd've\": \"you all would have\", \"y'all're\": \"you all are\", \"y'all've\": \"you all have\",\n",
        "                \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\", 'colour': 'color',\n",
        "                'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor',\n",
        "                'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', '2F4U':\t'Too Fast For You',\n",
        "                'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I',\n",
        "                'theBest': 'the best', 'howdoes': 'how does', 'Etherium': 'Ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what',\n",
        "                'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization', 'AKA':\t'also known as', 'ACK':\t'Acknowledgment', 'AFAIK':\t'As far as I know', 'AFAIR':\t'As far as I remember'}\n",
        "\n",
        "abbr = {k.lower(): v.lower() for k, v in abbr.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_O7owDxVuHk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize_word(word):\n",
        "    temp = word\n",
        "    while True:\n",
        "        w = re.sub(r\"([a-zA-Z])\\1\\1\", r\"\\1\\1\", temp)\n",
        "        if (w == temp):\n",
        "            break\n",
        "        else:\n",
        "            temp = w\n",
        "    return w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRoGzLSAVyYa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(text,abbreviation_dict=abbr, stem=False):\n",
        "    # Remove link,user and special characters\n",
        "    text = re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()\n",
        "\n",
        "    filtered_text=[]\n",
        "    for t in text.split(' '):\n",
        "        if (t in abbreviation_dict):\n",
        "            tok = abbreviation_dict.get(t).split(' ')\n",
        "            filtered_text.extend(tok)\n",
        "        else:\n",
        "            filtered_text.append(t)\n",
        "\n",
        "    tokee=[normalize_word(word) for word in filtered_text]\n",
        "    tokee=[t for t in tokee if t not in stop_words]\n",
        "\n",
        "    tokens = []\n",
        "    for token in tokee:\n",
        "        if stem:\n",
        "            tokens.append(stemmer.stem(token))\n",
        "        else:\n",
        "            tokens.append(token)\n",
        "    ll=['b','c','d','e','f','g','h','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']\n",
        "    tokens=[g for g in tokens if g not in ll]\n",
        "    if len(tokens)>=3:\n",
        "        return \" \".join(tokens)\n",
        "    else:\n",
        "        return float('NaN')\n",
        "df[5] = df[5].apply(lambda x: preprocess(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPq7MooP-hPU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df=df.dropna(axis=0,how='any')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YfQcSib-PZ9",
        "colab_type": "code",
        "outputId": "a9a114dd-c7e9-46ba-c0bc-42b07ebfba2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df[5])\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(\"Total words\", vocab_size)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total words 290795\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGRhdHKJA29c",
        "colab_type": "code",
        "outputId": "4fd66756-a2d0-4439-ba74-a16189dfdb86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "max([len(s.split(' ')) for s in list(df[5]) ])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sBm3rdo-ukF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = pad_sequences(tokenizer.texts_to_sequences(df[5]), maxlen=200)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iwM1Sxc-_eZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = df[0].unique().tolist()\n",
        "labels.append('NEUTRAL')\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(df[0].tolist())\n",
        "y = encoder.transform(df[0].tolist())\n",
        "y = y.reshape(-1,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pBuP_qjWbdX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "655152b7-8284-46c1-c34d-d2b492385612"
      },
      "source": [
        "embeddings_index = dict()\n",
        "f = open('/content/drive/My Drive/glove.6B.300d.txt')\n",
        "for line in f:\n",
        "\tvalues = line.split()\n",
        "\tword = values[0]\n",
        "\tcoefs = np.asarray(values[1:], dtype='float32')\n",
        "\tembeddings_index[word] = coefs\n",
        "f.close()\n",
        "print('Loaded %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSqcste3BB8K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a weight matrix for words in training docs\n",
        "embedding_matrix = np.zeros((vocab_size, 300))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "\tembedding_vector = embeddings_index.get(word)\n",
        "\tif embedding_vector is not None:\n",
        "\t\tembedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BenqKgoU_Y5Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout, Embedding, Flatten, Conv1D, MaxPooling1D, Embedding\n",
        "from tensorflow.keras import utils\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "import tensorflow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8Ev_nfcBFV_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_layer = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=200, trainable=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKb1rARZ_a9V",
        "colab_type": "code",
        "outputId": "f9710932-cea5-4e6d-b8f5-900359992c51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "model = Sequential()\n",
        "model.add(embedding_layer)\n",
        "model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
        "#model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Conv1D(filters=256, kernel_size=3, activation='relu'))\n",
        "#model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.summary()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 200, 300)          87238500  \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 198, 128)          115328    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 198, 128)          0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 196, 256)          98560     \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 50176)             0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 50176)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               6422656   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 93,875,173\n",
            "Trainable params: 6,636,673\n",
            "Non-trainable params: 87,238,500\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B73mZA9T_w6X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=\"adam\",\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voWVUCrq_yJ6",
        "colab_type": "code",
        "outputId": "742d00d7-2f5e-4027-915f-d66d2567dcfd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "history = model.fit(X,y,\n",
        "                    batch_size=2048,\n",
        "                    epochs=10,\n",
        "                    validation_split=0.2,\n",
        "                    verbose=1)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "552/552 [==============================] - 115s 208ms/step - loss: 0.4940 - accuracy: 0.7564 - val_loss: 0.5731 - val_accuracy: 0.7126\n",
            "Epoch 2/10\n",
            "552/552 [==============================] - 114s 207ms/step - loss: 0.4533 - accuracy: 0.7850 - val_loss: 0.6300 - val_accuracy: 0.6718\n",
            "Epoch 3/10\n",
            "552/552 [==============================] - 114s 207ms/step - loss: 0.4424 - accuracy: 0.7917 - val_loss: 0.5530 - val_accuracy: 0.7268\n",
            "Epoch 4/10\n",
            "552/552 [==============================] - 114s 207ms/step - loss: 0.4351 - accuracy: 0.7963 - val_loss: 0.5459 - val_accuracy: 0.7337\n",
            "Epoch 5/10\n",
            "552/552 [==============================] - 114s 207ms/step - loss: 0.4298 - accuracy: 0.7996 - val_loss: 0.5618 - val_accuracy: 0.7230\n",
            "Epoch 6/10\n",
            "552/552 [==============================] - 114s 207ms/step - loss: 0.4253 - accuracy: 0.8021 - val_loss: 0.5665 - val_accuracy: 0.7212\n",
            "Epoch 7/10\n",
            "552/552 [==============================] - 114s 207ms/step - loss: 0.4222 - accuracy: 0.8045 - val_loss: 0.6808 - val_accuracy: 0.6519\n",
            "Epoch 8/10\n",
            "552/552 [==============================] - 114s 207ms/step - loss: 0.4184 - accuracy: 0.8061 - val_loss: 0.5668 - val_accuracy: 0.7258\n",
            "Epoch 9/10\n",
            "552/552 [==============================] - 114s 207ms/step - loss: 0.4159 - accuracy: 0.8074 - val_loss: 0.5754 - val_accuracy: 0.7214\n",
            "Epoch 10/10\n",
            "552/552 [==============================] - 114s 207ms/step - loss: 0.4133 - accuracy: 0.8088 - val_loss: 0.5322 - val_accuracy: 0.7450\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFd8iCcskI6O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# PLOT MODEL\n",
        "tf.keras.utils.plot_model(\n",
        "    model,\n",
        "    to_file=\"model.png\",\n",
        "    show_shapes=True,\n",
        "    show_layer_names=False,\n",
        "    rankdir=\"TB\",\n",
        "    expand_nested=False,\n",
        "    dpi=96,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUMXOiIfkJH6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('/content/drive/My Drive/CNNsentiment.h5') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4obHkTuKkfUP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.models.load_model('/content/drive/My Drive/CNNsentiment.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1U35wSrjy5A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df1 = pd.read_csv('/content/drive/My Drive/sarcastic_comments_final.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93EUe9aBj7Z_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(text,abbreviation_dict=abbr, stem=False):\n",
        "    # Remove link,user and special characters\n",
        "    text = re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()\n",
        "\n",
        "    filtered_text=[]\n",
        "    for t in text.split(' '):\n",
        "        if (t in abbreviation_dict):\n",
        "            tok = abbreviation_dict.get(t).split(' ')\n",
        "            filtered_text.extend(tok)\n",
        "        else:\n",
        "            filtered_text.append(t)\n",
        "\n",
        "    tokee=[normalize_word(word) for word in filtered_text]\n",
        "    tokee=[t for t in tokee if t not in stop_words]\n",
        "\n",
        "    tokens = []\n",
        "    for token in tokee:\n",
        "        if stem:\n",
        "            tokens.append(stemmer.stem(token))\n",
        "        else:\n",
        "            tokens.append(token)\n",
        "    ll=['b','c','d','e','f','g','h','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']\n",
        "    tokens=[g for g in tokens if g not in ll]\n",
        "    if len(tokens)>=1:\n",
        "        return \" \".join(tokens)\n",
        "    else:\n",
        "        return float('NaN')\n",
        "df1.comment = df1.comment.apply(lambda x: preprocess(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uK3ItWG_j-vi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df1=df1.dropna(axis=0,how='any')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTxbQ8AVkB3l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X1 = pad_sequences(tokenizer.texts_to_sequences(df1.comment), maxlen=200)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCEjXvMHkHSz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow\n",
        "new_input = model.input\n",
        "hidden_layer = model.layers[-2].output\n",
        "\n",
        "features_extract_model = tensorflow.keras.Model(new_input, hidden_layer)\n",
        "features_extract_model.trainable=False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7CIHaI9kJm7",
        "colab_type": "code",
        "outputId": "7271cc32-627a-41a2-b595-63c90e07cff8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.executing_eagerly()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5mnHDV9kLbm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "I=[i for i in range(0,X1.shape[0],1000)]\n",
        "I.append(X1.shape[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omCF-UUjkOcq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NDF=pd.DataFrame(columns=[str(t) for t in range(128)])\n",
        "for i in range(len(I[0:-1])):\n",
        "    x=tf.convert_to_tensor(X1[I[i]:I[i+1]])\n",
        "    z=features_extract_model(x)\n",
        "    ndf=pd.DataFrame(z.numpy(),columns=[str(t) for t in range(128)])\n",
        "    lst = [NDF, ndf]\n",
        "    NDF = pd.concat(lst)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lck_WIYxkQmr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NDF.to_csv('/content/drive/My Drive/sentiment_extracted.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}